{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  0.4.0\n",
      "Torchvision Version:  0.2.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import csv\n",
    "from skimage import io\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "USE_CIFAR10 = True\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"squeezenet\"\n",
    "\n",
    "if USE_CIFAR10:\n",
    "    num_classes = 10\n",
    "else:\n",
    "    num_classes = 26\n",
    "num_epochs = 50\n",
    "\n",
    "feature_extract = True\n",
    "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs = 25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch,num_epochs-1))\n",
    "        print('-'*10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "\n",
    "                if not(USE_CIFAR10):\n",
    "                    labels = labels-1\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs,labels) #<questo Ã¨ se prendiamo in considerazione il model inception\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "         \n",
    "                    _, preds =  torch.max(outputs,1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss/len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double()/len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed//60,time_elapsed%60))\n",
    "    print('Best val acc: {:.4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#If i'm doing features extraction, I don't have to update the gradients of the model\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.1307,), (0.3081,))\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)            \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "#        train_dataset = datasets.CIFAR10('../data', train=True, download=True, transform=dataset_transform)\n",
    "#        test_dataset = datasets.CIFAR10('../data', train=False, download=True, transform=dataset_transform)\n",
    "        \n",
    "#        self.train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MARVEL_dataset(Dataset):\n",
    "    def __init__(self, dat_file,train = True, transform = None):   \n",
    "        self.root_dir = os.path.dirname(dat_file)\n",
    "        datContent = [i.strip().split(',') for i in open(dat_file).readlines()]\n",
    "        if train:\n",
    "            csv_file = os.path.join(self.root_dir, \"data_Train.csv\")\n",
    "        else:\n",
    "            csv_file = os.path.join(self.root_dir, \"data_Test.csv\")\n",
    "        with open(csv_file, \"w\") as f:\n",
    "            writer = csv.writer(f,delimiter=',')\n",
    "            writer.writerow([\"counter\", \"set\", \"class\", \"label\",\"location\"])\n",
    "            for line in datContent:\n",
    "                if train and line[1]=='1':\n",
    "                    if not(line[4] == '-'):\n",
    "                        writer.writerows([line])  \n",
    "                if not(train) and line[1] == '2':\n",
    "                    if not(line[4]=='-'):\n",
    "                        writer.writerows([line]) \n",
    "                \n",
    "        self.MARVEL_datafile = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.MARVEL_datafile)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name = self.MARVEL_datafile.iloc[idx,4]\n",
    "        image = self.__loadfile(img_name)\n",
    "        target = self.MARVEL_datafile.iloc[idx,2]\n",
    "        if self.transform:\n",
    "            image = Image.fromarray(image)\n",
    "            sample = self.transform(image)\n",
    "        else:\n",
    "            sample = image\n",
    "        return (sample,target)\n",
    "    \n",
    "    def __loadfile(self, data_file):\n",
    "        image = io.imread(data_file)\n",
    "        if len(image.shape)<3:\n",
    "            image = np.stack((image,)*3, axis=-1)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Initializing Datasets and Dataloaders...\n",
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "if USE_CIFAR10:\n",
    "    image_datasets = {'train': datasets.CIFAR10('../data', train=True, download=True, transform=dataset_transform),'val': datasets.CIFAR10('../data', train=True, download=True, transform=dataset_transform)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    # Create training and validation dataloaders\n",
    "    dataloaders_dict = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=4), 'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True, num_workers=4)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "else:\n",
    "    dat_file = \"/home/rita/JupyterProjects/EYE-SEA/DataSets/marveldataset2016-master/FINAL.dat\"\n",
    "    image_datasets = {'train': MARVEL_dataset(dat_file,train = True,transform=dataset_transform),'val': MARVEL_dataset(dat_file,train = False,transform=dataset_transform)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    # Create training and validation dataloaders\n",
    "    dataloaders_dict = {'train': DataLoader(image_datasets['train'], batch_size=100, shuffle=True) , 'val': DataLoader(image_datasets['val'], batch_size=100, shuffle=True)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.8845 Acc: 0.6966\n",
      "val Loss: 0.6201 Acc: 0.7873\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.6254 Acc: 0.7831\n",
      "val Loss: 0.5572 Acc: 0.8068\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.5851 Acc: 0.7963\n",
      "val Loss: 0.5382 Acc: 0.8117\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.5640 Acc: 0.8018\n",
      "val Loss: 0.4972 Acc: 0.8290\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.5500 Acc: 0.8079\n",
      "val Loss: 0.4951 Acc: 0.8286\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.5414 Acc: 0.8112\n",
      "val Loss: 0.4764 Acc: 0.8348\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.5328 Acc: 0.8138\n",
      "val Loss: 0.5053 Acc: 0.8226\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.5241 Acc: 0.8164\n",
      "val Loss: 0.4879 Acc: 0.8297\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.5200 Acc: 0.8180\n",
      "val Loss: 0.4675 Acc: 0.8390\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.5123 Acc: 0.8205\n",
      "val Loss: 0.4764 Acc: 0.8335\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.5109 Acc: 0.8215\n",
      "val Loss: 0.4530 Acc: 0.8442\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.5135 Acc: 0.8212\n",
      "val Loss: 0.4689 Acc: 0.8369\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.5080 Acc: 0.8228\n",
      "val Loss: 0.4577 Acc: 0.8409\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.5065 Acc: 0.8228\n",
      "val Loss: 0.4484 Acc: 0.8439\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.5054 Acc: 0.8231\n",
      "val Loss: 0.4408 Acc: 0.8488\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.5016 Acc: 0.8235\n",
      "val Loss: 0.4657 Acc: 0.8371\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.5014 Acc: 0.8255\n",
      "val Loss: 0.4423 Acc: 0.8481\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.4976 Acc: 0.8265\n",
      "val Loss: 0.4413 Acc: 0.8455\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.4986 Acc: 0.8256\n",
      "val Loss: 0.4471 Acc: 0.8448\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.4942 Acc: 0.8254\n",
      "val Loss: 0.4487 Acc: 0.8445\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.4945 Acc: 0.8255\n",
      "val Loss: 0.4336 Acc: 0.8508\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.4956 Acc: 0.8266\n",
      "val Loss: 0.4353 Acc: 0.8498\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.4944 Acc: 0.8282\n",
      "val Loss: 0.4482 Acc: 0.8440\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.4953 Acc: 0.8270\n",
      "val Loss: 0.4532 Acc: 0.8418\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.4919 Acc: 0.8273\n",
      "val Loss: 0.4359 Acc: 0.8486\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.4886 Acc: 0.8285\n",
      "val Loss: 0.4492 Acc: 0.8434\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.4921 Acc: 0.8276\n",
      "val Loss: 0.4394 Acc: 0.8463\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.4890 Acc: 0.8290\n",
      "val Loss: 0.4591 Acc: 0.8391\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.4895 Acc: 0.8291\n",
      "val Loss: 0.4250 Acc: 0.8532\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.4887 Acc: 0.8285\n",
      "val Loss: 0.4396 Acc: 0.8477\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.4872 Acc: 0.8294\n",
      "val Loss: 0.4338 Acc: 0.8486\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.4829 Acc: 0.8315\n",
      "val Loss: 0.4345 Acc: 0.8478\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.4856 Acc: 0.8299\n",
      "val Loss: 0.4412 Acc: 0.8466\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.4870 Acc: 0.8289\n",
      "val Loss: 0.4362 Acc: 0.8485\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.4826 Acc: 0.8320\n",
      "val Loss: 0.4279 Acc: 0.8509\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.4863 Acc: 0.8294\n",
      "val Loss: 0.4353 Acc: 0.8485\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.4851 Acc: 0.8306\n",
      "val Loss: 0.4236 Acc: 0.8532\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.4855 Acc: 0.8303\n",
      "val Loss: 0.4256 Acc: 0.8530\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.4813 Acc: 0.8323\n",
      "val Loss: 0.4391 Acc: 0.8473\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.4885 Acc: 0.8289\n",
      "val Loss: 0.4399 Acc: 0.8461\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.4831 Acc: 0.8301\n",
      "val Loss: 0.4386 Acc: 0.8458\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.4816 Acc: 0.8317\n",
      "val Loss: 0.4360 Acc: 0.8474\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.4842 Acc: 0.8305\n",
      "val Loss: 0.4335 Acc: 0.8486\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.4797 Acc: 0.8330\n",
      "val Loss: 0.4247 Acc: 0.8539\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.4829 Acc: 0.8309\n",
      "val Loss: 0.4231 Acc: 0.8538\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.4800 Acc: 0.8311\n",
      "val Loss: 0.4377 Acc: 0.8466\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.4843 Acc: 0.8297\n",
      "val Loss: 0.4209 Acc: 0.8536\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.4808 Acc: 0.8326\n",
      "val Loss: 0.4280 Acc: 0.8514\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.4802 Acc: 0.8313\n",
      "val Loss: 0.4217 Acc: 0.8530\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.4801 Acc: 0.8332\n",
      "val Loss: 0.4289 Acc: 0.8505\n",
      "\n",
      "Training complete in 73m 26s\n",
      "Best val acc: 0.8539\n"
     ]
    }
   ],
   "source": [
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHAT IF I WANT TO TRAIN A MODEL COMPLETELY?\n",
    "#THEN WE TRAIN FROM SCRATCH\n",
    "\n",
    "# Initialize the non-pretrained version of the model used for this run\n",
    "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs_1 for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
