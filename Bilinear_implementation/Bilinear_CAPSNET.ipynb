{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "from skimage import io\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import import_ipynb\n",
    "import ResNetCaps\n",
    "\n",
    "verbose = False\n",
    "USE_CUDA = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MNIST_USE = False\n",
    "CIFAR10_USE = True\n",
    "MARVEL_USE = False\n",
    "\n",
    "#matrix product for bilinear function\n",
    "euclidean = True\n",
    "kronecker = False\n",
    "outer_m = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MARVEL_dataset(Dataset):\n",
    "    def __init__(self, dat_file,train = True, transform = None):   \n",
    "        self.root_dir = os.path.dirname(dat_file)\n",
    "        datContent = [i.strip().split(',') for i in open(dat_file).readlines()]\n",
    "        if train:\n",
    "            csv_file = os.path.join(self.root_dir, \"data_Train.csv\")\n",
    "        else:\n",
    "            csv_file = os.path.join(self.root_dir, \"data_Test.csv\")\n",
    "        with open(csv_file, \"w\") as f:\n",
    "            writer = csv.writer(f,delimiter=',')\n",
    "            writer.writerow([\"counter\", \"set\", \"class\", \"label\",\"location\"])\n",
    "            for line in datContent:\n",
    "                if train and line[1]=='1':\n",
    "                    if not(line[4] == '-'):\n",
    "                        writer.writerows([line])  \n",
    "                if not(train) and line[1] == '2':\n",
    "                    if not(line[4]=='-'):\n",
    "                        writer.writerows([line]) \n",
    "                \n",
    "        self.MARVEL_datafile = pd.read_csv(csv_file)       \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.MARVEL_datafile)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name = self.MARVEL_datafile.iloc[idx,4]\n",
    "        image = self.__loadfile(img_name)\n",
    "        target = self.MARVEL_datafile.iloc[idx,2]\n",
    "        if self.transform:\n",
    "            image = Image.fromarray(image)\n",
    "            sample = self.transform(image)\n",
    "        else:\n",
    "            sample = image\n",
    "        return (sample,target)\n",
    "    \n",
    "    def __loadfile(self, data_file):\n",
    "        image = io.imread(data_file)\n",
    "        if len(image.shape)<3:\n",
    "            image = np.stack((image,)*3, axis=-1)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),        \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "if CIFAR10_USE: \n",
    "    NUM_CLASSES = 10\n",
    "    print(\"CIFAR10\")\n",
    "    image_datasets = {'train': datasets.CIFAR10('../data', train=True, download=True, transform=dataset_transform),'val': datasets.CIFAR10('../data', train=False, download=True, transform=dataset_transform)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "    dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True) }\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "if MARVEL_USE: \n",
    "    NUM_CLASSES = 26\n",
    "    print(\"MARVEL\")\n",
    "    dat_file = \"/home/rita/JupyterProjects/EYE-SEA/DataSets/marveldataset2016-master/FINAL.dat\"\n",
    "\n",
    "    image_datasets = {'train': MARVEL_dataset(dat_file,train = True,transform=dataset_transform),'val': MARVEL_dataset(dat_file,train = False,transform=dataset_transform)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "    dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True) }\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "if MNIST_USE: \n",
    "    dataset_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),        \n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "    NUM_CLASSES = 10\n",
    "    print(\"MNIST\")\n",
    "    image_datasets = {'train': datasets.MNIST('../data', train=True, download=True, transform=dataset_transform),'val': datasets.MNIST('../data', train=False, download=True, transform=dataset_transform)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "    dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True) }\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bilinear(nn.Module):\n",
    "    def __init__(self,NUM_CLASSES):\n",
    "        super(Bilinear, self).__init__()\n",
    "        #Features function :\n",
    "        self.modelCaps1 = ResNetCaps.ResNetCaps(NUM_CLASSES)\n",
    "        self.modelCaps2 = ResNetCaps.ResNetCaps(NUM_CLASSES)\n",
    "        #Classification function:\n",
    "        self.modelLin = nn.Linear(10, NUM_CLASSES)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        #1) Extract features functions vectors (I need to do MATRIX OUTER PRODUCT)\n",
    "        digit1, masked = self.modelCaps1(inputs)\n",
    "        digit2, masked = self.modelCaps2(inputs)\n",
    "        #2) Classification Function    \n",
    "        output = F.softmax(self.modelLin(self.bilinear(digit1, digit2)),dim=1)\n",
    "        \n",
    "        return output, masked\n",
    " \n",
    "    def bilinear(self, A, B):\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        \n",
    "        Z_list = []\n",
    "        labels_size = A.size()[1]\n",
    "        batch = min(A.size()[0],batch_size)\n",
    "        for i in range(batch):\n",
    "        \n",
    "            a = A[i,:,:,0]\n",
    "            b = B[i,:,:,0]\n",
    "            #1.1) Pooling for aggregation of features vectors \n",
    "            if euclidean:\n",
    "                #EUCLIDEAN MATRIX PRODUCT\n",
    "                if verbose: print(\"Dim A {} B {}\".format(a.shape,b.shape))\n",
    "                x = torch.mm(a,torch.transpose(b,0,1))\n",
    "                x = torch.sum(x, dim=1)\n",
    "            if outer_m:\n",
    "                #OUTER MATRIX PRODUCT\n",
    "                A = torch.sum(A,dim=1)\n",
    "                B = torch.sum(B,dim=1)\n",
    "                x = torch.ger(A,B)\n",
    "                x = torch.sum(x, dim=1)\n",
    "            if kronecker:\n",
    "                #KRONECKER MATRIX PRODUCT\n",
    "                x = torch.kron(A.cpu().numpy(),B.cpu().numpy())\n",
    "                x = torch.from_numpy(x).float().to(device)\n",
    "                \n",
    "            y = torch.sign(x)*torch.sqrt(torch.FloatTensor.abs_(x))\n",
    "            z = y/torch.norm(y)       \n",
    "            \n",
    "            Z_list.append(z)\n",
    "            \n",
    "        Z = torch.cat(Z_list,0)\n",
    "        Z = Z.view([batch,labels_size])\n",
    "        return Z    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model = Bilinear(10)\n",
    "model = model.to(device)\n",
    "\n",
    "#optimizers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr = 0.001)\n",
    "n_epochs = 10\n",
    "#train\n",
    "start = time.time()\n",
    "#batch_id = 100\n",
    "#inputs, labels = next(iter(dataloaders['train']))\n",
    "accuracy_train = []\n",
    "loss_train = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "    model.train() \n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "\n",
    "    print('epoch {}:{}'.format(epoch+1, n_epochs)) \n",
    "    for batch_id, (inputs, labels) in enumerate(dataloaders['train']):\n",
    "\n",
    "        labels =torch.eye(NUM_CLASSES).index_select(dim=0, index=labels)\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, masked = model(inputs)\n",
    "        if verbose: print(outputs)\n",
    "        if verbose: print(labels.long())\n",
    "        _,label = torch.max(labels, 1)\n",
    "        if verbose: print(label)\n",
    "        loss = criterion(outputs, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        train_accuracy += (sum(np.argmax(outputs.data.cpu().numpy(), 1) == np.argmax(labels.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"train accuracy:\", sum(np.argmax(outputs.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(labels.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "            if verbose: print(\"masked {}\".format(np.argmax(masked.data.cpu().numpy(), 1)))\n",
    "            if verbose: print(\"labels {}\".format(np.argmax(labels.data.cpu().numpy(), 1)))\n",
    "        #                batch_accuracy.append(sum(np.argmax(preds.data.cpu().numpy(), 1) == \n",
    "        #                                       np.argmax(labels.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "    accuracy_train.append(train_accuracy/len(dataloaders['train'])\n",
    "    loss_train.append(train_loss/len(dataloaders['train']))\n",
    "end = time.time()\n",
    "print(\"Training time execution {}\".format(end-start))\n",
    "print(\"Loss value for training phase: {}\".format(train_loss / len(dataloaders['train'])))\n",
    "print(\"Accuracy value for training phase: {}\".format(train_accuracy / len(dataloaders['train'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1,n_epochs+1)\n",
    "plt.plot(epochs, loss_train, color='g')\n",
    "plt.plot(epochs, accuracy_train, color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training phase')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "test_accuracy = 0 \n",
    "start = time.time()\n",
    "for batch_id, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "\n",
    "    labels = torch.eye(NUM_CLASSES).index_select(dim=0, index=labels)\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "    if USE_CUDA: inputs, labels = inputs.to(device), labels.to(device)#cuda()\n",
    "\n",
    "    outputs, masked = model(inputs)\n",
    "    _,label = torch.max(labels, 1)\n",
    "    loss = criterion(outputs, label.long())\n",
    "\n",
    "    test_loss += loss.data[0]\n",
    "    test_accuracy += (sum(np.argmax(outputs.data.cpu().numpy(), 1) == np.argmax(labels.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"test accuracy:\", sum(np.argmax(outputs.data.cpu().numpy(), 1) == \n",
    "                               np.argmax(labels.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "end = time.time()\n",
    "print(\"Test time execution {}\".format(end-start))\n",
    "print(\"Loss value for test phase: {}\".format(test_loss /  len(dataloaders['val']))) \n",
    "print(\"Accuracy value for test phase: {}\".format(test_accuracy /  len(dataloaders['val'])))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
