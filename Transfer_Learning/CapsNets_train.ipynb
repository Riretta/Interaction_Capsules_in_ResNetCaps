{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "from skimage import io\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "verbose = False\n",
    "USE_CUDA = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import import_ipynb\n",
    "import CapsNet_Layers \n",
    "\n",
    "def lr_decrease(optimizer, lr_clip):  \n",
    "    for param_group in optimizer.param_groups:\n",
    "        init_lr = param_group['lr'] \n",
    "        param_group['lr'] = init_lr*lr_clip\n",
    "        \n",
    "def isnan(x):\n",
    "    return x != x   \n",
    "\n",
    "dataset_transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),        \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "NUM_CLASSES = 10\n",
    "print(\"CIFAR10\")\n",
    "image_datasets = {'train': datasets.CIFAR10('../data', train=True, download=True, transform=dataset_transform),'val': datasets.CIFAR10('../data', train=False, download=True, transform=dataset_transform)}\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True) }\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=> using model CapsuleNET\")\n",
    "model = CapsNet_Layers.CapsNet(NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.to(device)#cuda()\n",
    "    print('cuda')\n",
    "optimizer = Adam(model.parameters(),lr = 0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(torch.version)\n",
    "\n",
    "n_epochs = 10\n",
    "x = range(0,n_epochs)\n",
    "accuracy_train = []\n",
    "loss_train = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "    capsule_net.train() \n",
    "    train_loss = 0 \n",
    "    train_accuracy = 0\n",
    "    \n",
    "    batch_accuracy = []\n",
    "\n",
    "    print('epoch {}:{}'.format(epoch+1, n_epochs)) \n",
    "    for batch_id, (data, target) in enumerate(dataloaders['train']):\n",
    "        target =torch.eye(NUM_CLASSES).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.to(device), target.to(device)#.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "        \n",
    "        if isnan(loss):\n",
    "            Data_name_file = \"data_eq\"+str(epoch)+\"_batch_id\"+str(batch_id)+\".pt\"\n",
    "            Output_name_file = \"output_eq\"+str(epoch)+\"_batch_id\"+str(batch_id)+\".pt\"\n",
    "            break\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        train_accuracy += (sum(np.argmax(masked.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "        \n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"train accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "            print(\"loss {}\".format(loss.data[0]))\n",
    "            batch_accuracy.append(sum(np.argmax(masked.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "    accuracy_train.append(np.mean(batch_accuracy))\n",
    "    del batch_accuracy\n",
    "    loss_train.append(train_loss/len(dataloaders['train']))\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Training time execution {}\".format(end-start))\n",
    "print(\"Loss value for test phase: {}\".format(train_loss /  len(dataloaders['train'])))\n",
    "print(\"Accuracy value for test phase: {}\".format(train_accuracy /  len(dataloaders['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
