{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ResNetCaps.ipynb\n",
      "importing Jupyter notebook from ResNetCaps_IBN.ipynb\n",
      "importing Jupyter notebook from CapsNet_Layers.ipynb\n",
      "importing Jupyter notebook from DenseNetCaps.ipynb\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "from skimage import io\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "verbose = False\n",
    "USE_CUDA = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ibn = False\n",
    "\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import import_ipynb\n",
    "import ResNetCaps\n",
    "import ResNetCaps_IBN\n",
    "import DenseNetCaps\n",
    "\n",
    "\n",
    "def resume_model(name_file, model, optimizer): \n",
    "    if os.path.isfile(name_file):\n",
    "        print(\"=> loading checkpoint '{}'\".format(name_file))\n",
    "        checkpoint = torch.load(name_file)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(name_file, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(name_file))\n",
    "\n",
    "    return start_epoch,model,optimizer\n",
    "\n",
    "def shift_left_once(A):\n",
    "    a =  []\n",
    "    AD = A.detach().cpu().numpy()\n",
    "    temp = AD[0,:]\n",
    "\n",
    "    for index in range(1,len(AD)):\n",
    "        a = np.concatenate([a,AD[index]])\n",
    "    a = np.concatenate([a,temp])\n",
    "    a_tensor = torch.from_numpy(a)\n",
    "    return ((a_tensor.view(len(a),1)).float()).to(device)\n",
    "\n",
    "def shift_left(lst, n):\n",
    "    assert (n >= 0), \"n should be non-negative integer\"\n",
    "    for _ in range(n):\n",
    "        shift_left_once(lst)\n",
    "\n",
    "def circulant(A):\n",
    "    r = A.size()[0]\n",
    "    circ_list_a = []\n",
    "    circ_list_a.append(A)\n",
    "    \n",
    "    a = shift_left_once(A)\n",
    "    for i in range(r-1):   \n",
    "        circ_list_a.append(a)      \n",
    "        a = shift_left_once(a)\n",
    "\n",
    "    \n",
    "    circ = torch.cat(circ_list_a,0).detach().requires_grad_()\n",
    "    circ = circ.view([r,r])\n",
    "    if verbose: print(\"circular matrix {}\".format(circ))\n",
    "    return circ.to(device)\n",
    "        \n",
    "    \n",
    "class Interaction(nn.Module):\n",
    "    def __init__(self,NUM_CLASSES,batch_size,not_ibn=True,model_path= \" \", model_name='ResNetCaps'):\n",
    "        super(Interaction, self).__init__()\n",
    "        self.not_ibn = not_ibn ##########################da modificare\n",
    "        if not_ibn:\n",
    "            print(model_name)\n",
    "            if model_name == 'ResNetCaps':\n",
    "                print(\"sibling ResNetCaps\")\n",
    "                self.modelCaps1 = ResNetCaps.ResNetCaps(NUM_CLASSES)\n",
    "                self.modelCaps2 = ResNetCaps.ResNetCaps(NUM_CLASSES)\n",
    "            else:\n",
    "                print(\"sibling DenseNetCaps\")\n",
    "                self.modelCaps1 = DenseNetCaps.DenseNetCaps(NUM_CLASSES)\n",
    "                self.modelCaps2 = DenseNetCaps.DenseNetCaps(NUM_CLASSES)                \n",
    "        else:\n",
    "            print(\"sibling ResNetCaps_IBN\")\n",
    "            model =  ResNetCaps_IBN.IBN_ResNetCaps(NUM_CLASSES)\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr = 0.001)\n",
    "            start_epoch,model,optimizer = resume_model(model_path, model, optimizer)\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.modelCaps1 = model\n",
    "            self.modelCaps2 = model\n",
    "        \n",
    "        self.W1 = torch.randn(NUM_CLASSES,NUM_CLASSES)\n",
    "        self.W1.requires_grad = True\n",
    "        \n",
    "        self.W2 = torch.randn(NUM_CLASSES,NUM_CLASSES)\n",
    "        self.W2.requires_grad = True\n",
    "        \n",
    "        self.modelLin = nn.Linear(NUM_CLASSES, NUM_CLASSES)\n",
    "        self.NClass = NUM_CLASSES\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def cuda(self, device=None):\n",
    "        self = super().cuda(device)\n",
    "        self.W1 = self.W1.cuda(device)\n",
    "        self.W2 = self.W2.cuda(device)\n",
    "        return self \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        if self.not_ibn:\n",
    "            digit1,_ = self.modelCaps1(inputs)\n",
    "            digit2,_ = self.modelCaps2(inputs)\n",
    "        else:\n",
    "            digit1 = self.modelCaps1(inputs)\n",
    "            digit2 = self.modelCaps2(inputs)   \n",
    "        \n",
    "        output = self.modelLin(self.interaction(digit1,digit2))\n",
    "        return output\n",
    "\n",
    "    def model_loss(self, x, labels):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        v_c = x\n",
    "        if verbose: print(\"v_c {}\".format(v_c.size()))\n",
    "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
    "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
    "\n",
    "        loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def interaction(self,x,y):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "         \n",
    "        Z_list = []\n",
    "        labels_size = x.size()[1]\n",
    "        batch = min(x.size()[0],self.batch_size)\n",
    "        for i in range(batch):\n",
    "        \n",
    "            xi = torch.sum(x[i,:,:,0],dim=1)\n",
    "            yi = torch.sum(y[i,:,:,0],dim=1)\n",
    "            \n",
    "            xi = xi.view(self.NClass,1)\n",
    "            yi = yi.view(self.NClass,1)\n",
    "\n",
    "            V = torch.mm(self.W1,xi)   #< questo fa batch-matrix con matrix ?\n",
    "            C = torch.mm(self.W2,yi)\n",
    "\n",
    "            #CIRCOLANTE\n",
    "            A = circulant(V)\n",
    "            B = circulant(C)\n",
    "\n",
    "            #INTERACTION MOMENT\n",
    "            F = torch.mm(B,V)\n",
    "            G = torch.mm(A,C)\n",
    "            \n",
    "            M = torch.add(F,G)\n",
    "\n",
    "            Z_list.append(M)\n",
    "            \n",
    "        Z = torch.cat(Z_list,0)\n",
    "        if verbose : print(Z.size())\n",
    "        Z = Z.view(batch,labels_size)\n",
    "        \n",
    "        return Z   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
